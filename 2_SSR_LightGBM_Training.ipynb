{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41e3e0ca",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook trains the LightGBM ensemble using the feature vectors generated in LightGBM_DeepFeatures_ssr.ipynb. The Optuna framework is employed to optimize hyperparameters and identify the best-performing configuration.\n",
    "\n",
    "https://jiangliu5.github.io/imqac.github.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dba7507",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b3f567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import tensorflow as tf\n",
    "import random\n",
    "import optuna\n",
    "import sklearn\n",
    "import lightgbm\n",
    "from pathlib import Path\n",
    "from lightgbm import LGBMRegressor, early_stopping\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6555499",
   "metadata": {},
   "source": [
    "# Setting Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea443bb-470a-47e5-8f4d-341abf4e4d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some constants\n",
    "NUM_WORKERS = 0 #os.cpu_count()\n",
    "AMOUNT_TO_GET = 1.0\n",
    "SEED = 42\n",
    "\n",
    "# Define target data directory\n",
    "BASELINE_NAME = f\"VCIP_IMQA/VCIP\"\n",
    "BASELINE = Path(BASELINE_NAME)\n",
    "TARGET_DIR = BASELINE / \"EQ420_image\"\n",
    "TARGET_LABEL = BASELINE / \"Labels\"\n",
    "TARGET_BASE = BASELINE / \"IMQA\"\n",
    "\n",
    "# Setup training and test directories\n",
    "TARGET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create target model directory\n",
    "MODEL_DIR = \"trained_3072to128_updated\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Python's built-in random module\n",
    "random.seed(SEED)\n",
    "\n",
    "# NumPy\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# TensorFlow\n",
    "#tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c279df5f",
   "metadata": {},
   "source": [
    "# Specifying Target Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081898da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for available GPUs\n",
    "#gpus = tf.config.list_physical_devices('GPU')\n",
    "#if gpus:\n",
    "#    print(f\"GPUs detected: {gpus}\")\n",
    "#else:\n",
    "#    print(\"No GPU detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a1291e",
   "metadata": {},
   "source": [
    "# Extracting Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4637b583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant definition\n",
    "N_SPLITS = 8\n",
    "BATCH_SIZE = 1\n",
    "train_csv = pd.read_csv(TARGET_LABEL / 'mos_fold_train.csv').sample(frac=1)\n",
    "test_csv = pd.read_csv(TARGET_LABEL / 'mos_fold_test.csv').sample(frac=1)\n",
    "train_vector = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "test_vector = [9, 10]\n",
    "\n",
    "train_df_orig = pd.read_csv('train_features.csv')\n",
    "\n",
    "display(train_df_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9cdf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "n_components = 128\n",
    "pca = PCA(n_components=n_components, random_state=SEED)\n",
    "X_pca = pca.fit_transform(train_df_orig.drop(columns=['fold', 'mos'])) #Remove labels and fold!!, IMPORTANT!!!\n",
    "\n",
    "# Explained variance\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = explained_variance.cumsum()\n",
    "print(f\"Explained variance (128 components): {cumulative_variance[-1]:.4f}\")\n",
    "\n",
    "# Generate column names like PCA_0, PCA_1, ..., PCA_127\n",
    "pca_columns = [f'PCA_{i}' for i in range(n_components)]\n",
    "train_df = pd.DataFrame(X_pca, columns=pca_columns, index=train_df_orig.index)\n",
    "train_df['fold'] = train_df_orig['fold']\n",
    "train_df['mos'] = train_df_orig['mos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9b4268",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.drop(columns=['fold', 'mos']).max().max())\n",
    "print(train_df.drop(columns=['fold', 'mos']).min().min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a67bbb",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65486cfe",
   "metadata": {},
   "source": [
    "## First Round (2000 trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca9b5a0",
   "metadata": {},
   "source": [
    "### Maximizing R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4216ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "# Parameters\n",
    "threshold = 0.25\n",
    "z = 0.5\n",
    "\n",
    "time_results = [] \n",
    "\n",
    "for study_n in range(0, 8):\n",
    "\n",
    "    study_start = time.time()\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_vector)):\n",
    "\n",
    "        fold_start = time.time()\n",
    "\n",
    "        print(f\"\\nüîÅ Study {study_n + 1} - Optimizing fold {fold + 1}/{N_SPLITS}...\")\n",
    "\n",
    "        # Prepare fold data\n",
    "        fold_train = [train_vector[i] for i in train_idx]\n",
    "        fold_val = train_vector[val_idx[0]]\n",
    "        train_ids = train_df[train_df['fold'].isin(fold_train)]\n",
    "        val_ids = train_df.loc[train_df['fold'] == fold_val]\n",
    "\n",
    "        X_train_fold = train_ids.drop(columns=['fold', 'mos'])\n",
    "        y_train_fold = train_ids['mos']\n",
    "        X_val_fold = val_ids.drop(columns=['fold', 'mos'])\n",
    "        y_val_fold = val_ids['mos']\n",
    "        best_score = -float(\"inf\")\n",
    "\n",
    "        def fold_objective(trial):\n",
    "\n",
    "            global best_score\n",
    "            \n",
    "            # first round\n",
    "            lgb_model = LGBMRegressor(\n",
    "                random_state=SEED + study_n * 10 + fold,\n",
    "                verbosity=-1,\n",
    "                objective='regression',\n",
    "                boosting_type='gbdt',\n",
    "\n",
    "                # Less trees, more regularization\n",
    "                n_estimators=trial.suggest_int('n_estimators', 50, 200),\n",
    "                learning_rate=trial.suggest_float('learning_rate', 0.001, 0.5, log=True),\n",
    "\n",
    "                # Regularization\n",
    "                reg_alpha=trial.suggest_float('reg_alpha', 1e-4, 1.0, log=True),\n",
    "                reg_lambda=trial.suggest_float('reg_lambda', 1e-4, 1.0, log=True),\n",
    "\n",
    "                # Conservative model complexity\n",
    "                max_depth=trial.suggest_int('max_depth', 3, 8),\n",
    "                num_leaves=trial.suggest_int('num_leaves', 4, 24),\n",
    "\n",
    "                # Row and feature sampling\n",
    "                colsample_bytree=trial.suggest_float('colsample_bytree', 0.7, 1.0),\n",
    "                subsample=trial.suggest_float('subsample', 0.6, 0.9),\n",
    "                subsample_freq=1,\n",
    "\n",
    "                # Leaf-wise control\n",
    "                min_child_samples=trial.suggest_int('min_child_samples', 10, 60),\n",
    "\n",
    "                # Optional new tuning dimensions:\n",
    "                max_bin=trial.suggest_categorical('max_bin', [255, 512, 768]),\n",
    "\n",
    "                #device='gpu'\n",
    "            )            \n",
    "            \n",
    "            lgb_model.fit(\n",
    "                X_train_fold,\n",
    "                y_train_fold,\n",
    "                #sample_weight=smooth_weights(y_train_fold.values),\n",
    "                eval_set=[(X_val_fold, y_val_fold)],\n",
    "                eval_metric='rmse',\n",
    "                callbacks=[early_stopping(stopping_rounds=30,  verbose=False)])\n",
    "            preds = lgb_model.predict(X_val_fold)\n",
    "            score = r2_score(y_val_fold, preds)\n",
    "\n",
    "            # Save best model\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                joblib.dump(lgb_model, f\"{MODEL_DIR}/profiling_lgbm_model_fold_r2_{fold+1}_{study_n+1}_1.pkl\")\n",
    "                \n",
    "            return score\n",
    "\n",
    "        # Run optimization for the current fold\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(fold_objective, n_trials=1000, n_jobs=1)\n",
    "        best_params = study.best_trial.params\n",
    "        best_model = joblib.load(f\"{MODEL_DIR}/profiling_lgbm_model_fold_r2_{fold+1}_{study_n+1}_1.pkl\")  \n",
    "        preds_best = best_model.predict(X_val_fold)\n",
    "\n",
    "        print(f\"‚úÖ Best R¬≤: {r2_score(y_val_fold, preds_best):.4f}\")\n",
    "        print(f\"‚úÖ Fold {fold + 1} best R¬≤: {study.best_value:.4f}\")\n",
    "        print(f\"üõ†Ô∏è  Best hyperparameters: {best_params}\")\n",
    "\n",
    "        fold_end = time.time()\n",
    "        fold_time = (fold_end - fold_start) / 60\n",
    "\n",
    "        # save into results\n",
    "        time_results.append({\n",
    "            \"Study\": study_n + 1,\n",
    "            \"Fold\": fold + 1,\n",
    "            \"Training Time (min)\": fold_time\n",
    "        })\n",
    "\n",
    "        print(f\"‚è±Ô∏è Fold {fold + 1} training time: {fold_time:.2f} min\")\n",
    "    \n",
    "    study_end = time.time()\n",
    "    study_time = (study_end - study_start) / 60\n",
    "    print(f\"üìä Study {study_n + 1} total training time: {study_time:.2f} min\")\n",
    "\n",
    "# Convert time_results to dataframe\n",
    "df_times = pd.DataFrame(time_results)\n",
    "\n",
    "# Show results\n",
    "print(df_times)\n",
    "\n",
    "# Average per study\n",
    "avg_per_study = df_times.groupby(\"Study\")[\"Training Time (min)\"].mean().reset_index()\n",
    "print(avg_per_study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f57b54",
   "metadata": {},
   "source": [
    "### Minimizing MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addc04db",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "# Parameters\n",
    "threshold = 0.25\n",
    "z = 0.5\n",
    "\n",
    "for study_n in range(0, 8):\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_vector)):\n",
    "        print(f\"\\nüîÅ Study {study_n + 1} - Optimizing fold {fold + 1}/{N_SPLITS}...\")\n",
    "\n",
    "        # Prepare fold data\n",
    "        fold_train = [train_vector[i] for i in train_idx]\n",
    "        fold_val = train_vector[val_idx[0]]\n",
    "        train_ids = train_df[train_df['fold'].isin(fold_train)]\n",
    "        val_ids = train_df.loc[train_df['fold'] == fold_val]\n",
    "\n",
    "        X_train_fold = train_ids.drop(columns=['fold', 'mos'])\n",
    "        y_train_fold = train_ids['mos']\n",
    "        X_val_fold = val_ids.drop(columns=['fold', 'mos'])\n",
    "        y_val_fold = val_ids['mos']\n",
    "        best_score = float(\"inf\")\n",
    "\n",
    "        def fold_objective(trial):\n",
    "\n",
    "            global best_score\n",
    "            \n",
    "            # first round\n",
    "            lgb_model = LGBMRegressor(\n",
    "                random_state=SEED + study_n * 10 + fold,\n",
    "                verbosity=-1,\n",
    "                objective='regression',\n",
    "                boosting_type='gbdt',\n",
    "\n",
    "                # Less trees, more regularization\n",
    "                n_estimators=trial.suggest_int('n_estimators', 50, 200),\n",
    "                learning_rate=trial.suggest_float('learning_rate', 0.001, 0.5, log=True),\n",
    "\n",
    "                # Regularization\n",
    "                reg_alpha=trial.suggest_float('reg_alpha', 1e-4, 1.0, log=True),\n",
    "                reg_lambda=trial.suggest_float('reg_lambda', 1e-4, 1.0, log=True),\n",
    "\n",
    "                # Conservative model complexity\n",
    "                max_depth=trial.suggest_int('max_depth', 3, 8),\n",
    "                num_leaves=trial.suggest_int('num_leaves', 4, 24),\n",
    "\n",
    "                # Row and feature sampling\n",
    "                colsample_bytree=trial.suggest_float('colsample_bytree', 0.7, 1.0),\n",
    "                subsample=trial.suggest_float('subsample', 0.6, 0.9),\n",
    "                subsample_freq=1,\n",
    "\n",
    "                # Leaf-wise control\n",
    "                min_child_samples=trial.suggest_int('min_child_samples', 10, 60),\n",
    "\n",
    "                # Optional new tuning dimensions:\n",
    "                max_bin=trial.suggest_categorical('max_bin', [255, 512, 768]),\n",
    "\n",
    "            )            \n",
    "            \n",
    "            lgb_model.fit(\n",
    "                X_train_fold,\n",
    "                y_train_fold,\n",
    "                #sample_weight=smooth_weights(y_train_fold.values),\n",
    "                eval_set=[(X_val_fold, y_val_fold)],\n",
    "                eval_metric='rmse',\n",
    "                callbacks=[early_stopping(stopping_rounds=30,  verbose=False)])\n",
    "            preds = lgb_model.predict(X_val_fold)\n",
    "            score = mean_squared_error(y_val_fold, preds)\n",
    "\n",
    "            # Save best model\n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                joblib.dump(lgb_model, f\"{MODEL_DIR}/lgbm_model_fold_mse_{fold+1}_{study_n+1}_1.pkl\")\n",
    "                \n",
    "            return score\n",
    "\n",
    "        # Run optimization for the current fold\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(fold_objective, n_trials=2000, n_jobs=1)\n",
    "        best_params = study.best_trial.params\n",
    "        best_model = joblib.load(f\"{MODEL_DIR}/lgbm_model_fold_mse_{fold+1}_{study_n+1}_1.pkl\")  \n",
    "        preds_best = best_model.predict(X_val_fold)\n",
    "\n",
    "        print(f\"‚úÖ Best mse: {mean_squared_error(y_val_fold, preds_best):.4f}\")\n",
    "        print(f\"‚úÖ Fold {fold + 1} best mse: {study.best_value:.4f}\")\n",
    "        print(f\"üõ†Ô∏è  Best hyperparameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b85f53d",
   "metadata": {},
   "source": [
    "## Second Round (1000 trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba23d68",
   "metadata": {},
   "source": [
    "### Maximizing R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834373bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "# Parameters\n",
    "threshold = 0.25\n",
    "z = 0.5\n",
    "\n",
    "for study_n in range(0, 8):\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_vector)):\n",
    "        print(f\"\\nüîÅ Study {study_n + 1} - Optimizing fold {fold + 1}/{N_SPLITS}...\")\n",
    "\n",
    "        # Prepare fold data\n",
    "        fold_train = [train_vector[i] for i in train_idx]\n",
    "        fold_val = train_vector[val_idx[0]]\n",
    "        train_ids = train_df[train_df['fold'].isin(fold_train)]\n",
    "        val_ids = train_df.loc[train_df['fold'] == fold_val]\n",
    "\n",
    "        X_train_fold = train_ids.drop(columns=['fold', 'mos'])\n",
    "        y_train_fold = train_ids['mos']\n",
    "        X_val_fold = val_ids.drop(columns=['fold', 'mos'])\n",
    "        y_val_fold = val_ids['mos']\n",
    "        best_score = -float(\"inf\")\n",
    "\n",
    "        def fold_objective(trial):\n",
    "\n",
    "            global best_score\n",
    "            \n",
    "            lgb_model = LGBMRegressor(\n",
    "                random_state=SEED + study_n * 10 + fold,\n",
    "                verbosity=-1,\n",
    "                objective='regression',\n",
    "                boosting_type='gbdt',\n",
    "\n",
    "                # Narrowed search for faster convergence\n",
    "                n_estimators=trial.suggest_int('n_estimators', 100, 200),\n",
    "                learning_rate=trial.suggest_float('learning_rate', 0.01, 1, log=True),\n",
    "\n",
    "                # Focused regularization\n",
    "                reg_alpha=trial.suggest_float('reg_alpha', 1e-3, 0.5, log=True),\n",
    "                reg_lambda=trial.suggest_float('reg_lambda', 1e-3, 0.5, log=True),\n",
    "\n",
    "                # Tight complexity control\n",
    "                max_depth=trial.suggest_int('max_depth', 3, 7),\n",
    "                num_leaves=trial.suggest_int('num_leaves', 6, 24),\n",
    "\n",
    "                # Sampling (still some flexibility)\n",
    "                colsample_bytree=trial.suggest_float('colsample_bytree', 0.75, 1.0),\n",
    "                subsample=trial.suggest_float('subsample', 0.7, 0.9),\n",
    "                subsample_freq=1,\n",
    "\n",
    "                # Leaf-wise control (slightly narrower)\n",
    "                min_child_samples=trial.suggest_int('min_child_samples', 20, 50),\n",
    "\n",
    "                # Optional new tuning dimensions:\n",
    "                max_bin=trial.suggest_categorical('max_bin', [255, 512, 768]),\n",
    "                #min_split_gain=trial.suggest_float('min_split_gain', 0.0, 0.1),\n",
    "            )            \n",
    "            \n",
    "            lgb_model.fit(\n",
    "                X_train_fold,\n",
    "                y_train_fold,\n",
    "                #sample_weight=smooth_weights(y_train_fold.values),\n",
    "                eval_set=[(X_val_fold, y_val_fold)],\n",
    "                eval_metric='rmse',\n",
    "                callbacks=[early_stopping(stopping_rounds=30,  verbose=False)])\n",
    "            preds = lgb_model.predict(X_val_fold)\n",
    "            score = r2_score(y_val_fold, preds)\n",
    "\n",
    "            # Save best model\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                joblib.dump(lgb_model, f\"{MODEL_DIR}/lgbm_model_fold_r2_{fold+1}_{study_n+1}_2.pkl\")\n",
    "                \n",
    "            return score\n",
    "\n",
    "        # Run optimization for the current fold\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(fold_objective, n_trials=1000, n_jobs=1)\n",
    "        best_params = study.best_trial.params\n",
    "        best_model = joblib.load(f\"{MODEL_DIR}/lgbm_model_fold_r2_{fold+1}_{study_n+1}_2.pkl\")  \n",
    "        preds_best = best_model.predict(X_val_fold)\n",
    "\n",
    "        print(f\"‚úÖ Best R¬≤: {r2_score(y_val_fold, preds_best):.4f}\")\n",
    "        print(f\"‚úÖ Fold {fold + 1} best R¬≤: {study.best_value:.4f}\")\n",
    "        print(f\"üõ†Ô∏è  Best hyperparameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f41c088",
   "metadata": {},
   "source": [
    "### Minimizing MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1c5429",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "# Parameters\n",
    "threshold = 0.25\n",
    "z = 0.5\n",
    "\n",
    "for study_n in range(0, 8):\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_vector)):\n",
    "        print(f\"\\nüîÅ Study {study_n + 1} - Optimizing fold {fold + 1}/{N_SPLITS}...\")\n",
    "\n",
    "        # Prepare fold data\n",
    "        fold_train = [train_vector[i] for i in train_idx]\n",
    "        fold_val = train_vector[val_idx[0]]\n",
    "        train_ids = train_df[train_df['fold'].isin(fold_train)]\n",
    "        val_ids = train_df.loc[train_df['fold'] == fold_val]\n",
    "\n",
    "        X_train_fold = train_ids.drop(columns=['fold', 'mos'])\n",
    "        y_train_fold = train_ids['mos']\n",
    "        X_val_fold = val_ids.drop(columns=['fold', 'mos'])\n",
    "        y_val_fold = val_ids['mos']\n",
    "        best_score = float(\"inf\")\n",
    "\n",
    "        def fold_objective(trial):\n",
    "\n",
    "            global best_score\n",
    "            \n",
    "            lgb_model = LGBMRegressor(\n",
    "                random_state=SEED + study_n * 10 + fold,\n",
    "                verbosity=-1,\n",
    "                objective='regression',\n",
    "                boosting_type='gbdt',\n",
    "\n",
    "                # Narrowed search for faster convergence\n",
    "                n_estimators=trial.suggest_int('n_estimators', 100, 200),\n",
    "                learning_rate=trial.suggest_float('learning_rate', 0.01, 1, log=True),\n",
    "\n",
    "                # Focused regularization\n",
    "                reg_alpha=trial.suggest_float('reg_alpha', 1e-3, 0.5, log=True),\n",
    "                reg_lambda=trial.suggest_float('reg_lambda', 1e-3, 0.5, log=True),\n",
    "\n",
    "                # Tight complexity control\n",
    "                max_depth=trial.suggest_int('max_depth', 3, 7),\n",
    "                num_leaves=trial.suggest_int('num_leaves', 6, 24),\n",
    "\n",
    "                # Sampling (still some flexibility)\n",
    "                colsample_bytree=trial.suggest_float('colsample_bytree', 0.75, 1.0),\n",
    "                subsample=trial.suggest_float('subsample', 0.7, 0.9),\n",
    "                subsample_freq=1,\n",
    "\n",
    "                # Leaf-wise control (slightly narrower)\n",
    "                min_child_samples=trial.suggest_int('min_child_samples', 20, 50),\n",
    "\n",
    "                # Optional new tuning dimensions:\n",
    "                max_bin=trial.suggest_categorical('max_bin', [255, 512, 768]),\n",
    "                #min_split_gain=trial.suggest_float('min_split_gain', 0.0, 0.1),\n",
    "            )            \n",
    "            \n",
    "            lgb_model.fit(\n",
    "                X_train_fold,\n",
    "                y_train_fold,\n",
    "                #sample_weight=smooth_weights(y_train_fold.values),\n",
    "                eval_set=[(X_val_fold, y_val_fold)],\n",
    "                eval_metric='rmse',\n",
    "                callbacks=[early_stopping(stopping_rounds=30,  verbose=False)])\n",
    "            preds = lgb_model.predict(X_val_fold)\n",
    "            score = mean_squared_error(y_val_fold, preds)\n",
    "\n",
    "            # Save best model\n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                joblib.dump(lgb_model, f\"{MODEL_DIR}/lgbm_model_fold_mse_{fold+1}_{study_n+1}_2.pkl\")\n",
    "                \n",
    "            return score\n",
    "\n",
    "        # Run optimization for the current fold\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(fold_objective, n_trials=1000, n_jobs=1)\n",
    "        best_params = study.best_trial.params\n",
    "        best_model = joblib.load(f\"{MODEL_DIR}/lgbm_model_fold_mse_{fold+1}_{study_n+1}_2.pkl\")  \n",
    "        preds_best = best_model.predict(X_val_fold)\n",
    "\n",
    "        print(f\"‚úÖ Best mse: {mean_squared_error(y_val_fold, preds_best):.4f}\")\n",
    "        print(f\"‚úÖ Fold {fold + 1} best mse: {study.best_value:.4f}\")\n",
    "        print(f\"üõ†Ô∏è  Best hyperparameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bfab62",
   "metadata": {},
   "source": [
    "# Third Round (1000 trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d2a1b5",
   "metadata": {},
   "source": [
    "### Maximizing R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b12712",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "# Parameters\n",
    "threshold = 0.25\n",
    "z = 0.5\n",
    "\n",
    "for study_n in range(0, 8):\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_vector)):\n",
    "        print(f\"\\nüîÅ Study {study_n + 1} - Optimizing fold {fold + 1}/{N_SPLITS}...\")\n",
    "\n",
    "        # Prepare fold data\n",
    "        fold_train = [train_vector[i] for i in train_idx]\n",
    "        fold_val = train_vector[val_idx[0]]\n",
    "        train_ids = train_df[train_df['fold'].isin(fold_train)]\n",
    "        val_ids = train_df.loc[train_df['fold'] == fold_val]\n",
    "\n",
    "        X_train_fold = train_ids.drop(columns=['fold', 'mos'])\n",
    "        y_train_fold = train_ids['mos']\n",
    "        X_val_fold = val_ids.drop(columns=['fold', 'mos'])\n",
    "        y_val_fold = val_ids['mos']\n",
    "        best_score = -float(\"inf\")\n",
    "\n",
    "        def fold_objective(trial):\n",
    "\n",
    "            global best_score\n",
    "            \n",
    "            lgb_model = LGBMRegressor(\n",
    "                random_state=SEED + study_n * 10 + fold,\n",
    "                verbosity=-1,\n",
    "                objective='regression',\n",
    "                boosting_type='gbdt',\n",
    "\n",
    "                # Narrowed search for faster convergence\n",
    "                n_estimators=trial.suggest_int('n_estimators', 100, 200),\n",
    "                learning_rate=trial.suggest_float('learning_rate', 0.01, 1, log=True),\n",
    "\n",
    "                # Focused regularization\n",
    "                reg_alpha=trial.suggest_float('reg_alpha', 1e-3, 0.5, log=True),\n",
    "                reg_lambda=trial.suggest_float('reg_lambda', 1e-3, 0.5, log=True),\n",
    "\n",
    "                # Tight complexity control\n",
    "                max_depth=trial.suggest_int('max_depth', 3, 7),\n",
    "                num_leaves=trial.suggest_int('num_leaves', 6, 24),\n",
    "\n",
    "                # Sampling (still some flexibility)\n",
    "                colsample_bytree=trial.suggest_float('colsample_bytree', 0.75, 1.0),\n",
    "                subsample=trial.suggest_float('subsample', 0.7, 0.9),\n",
    "                subsample_freq=1,\n",
    "\n",
    "                # Leaf-wise control (slightly narrower)\n",
    "                min_child_samples=trial.suggest_int('min_child_samples', 20, 50),\n",
    "\n",
    "                # Optional new tuning dimensions:\n",
    "                max_bin=trial.suggest_categorical('max_bin', [255, 512, 768]),\n",
    "                min_split_gain=trial.suggest_float('min_split_gain', 0.0, 0.1),\n",
    "            )            \n",
    "            \n",
    "            lgb_model.fit(\n",
    "                X_train_fold,\n",
    "                y_train_fold,\n",
    "                #sample_weight=smooth_weights(y_train_fold.values),\n",
    "                eval_set=[(X_val_fold, y_val_fold)],\n",
    "                eval_metric='rmse',\n",
    "                callbacks=[early_stopping(stopping_rounds=30,  verbose=False)])\n",
    "            preds = lgb_model.predict(X_val_fold)\n",
    "            score = r2_score(y_val_fold, preds)\n",
    "\n",
    "            # Save best model\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                joblib.dump(lgb_model, f\"{MODEL_DIR}/lgbm_model_fold_r2_{fold+1}_{study_n+1}_3.pkl\")\n",
    "                \n",
    "            return score\n",
    "\n",
    "        # Run optimization for the current fold\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(fold_objective, n_trials=1000, n_jobs=1)\n",
    "        best_params = study.best_trial.params\n",
    "        best_model = joblib.load(f\"{MODEL_DIR}/lgbm_model_fold_r2_{fold+1}_{study_n+1}_3.pkl\")  \n",
    "        preds_best = best_model.predict(X_val_fold)\n",
    "\n",
    "        print(f\"‚úÖ Best R¬≤: {r2_score(y_val_fold, preds_best):.4f}\")\n",
    "        print(f\"‚úÖ Fold {fold + 1} best R¬≤: {study.best_value:.4f}\")\n",
    "        print(f\"üõ†Ô∏è  Best hyperparameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ca4bc0",
   "metadata": {},
   "source": [
    "### Minimizing MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c06f724",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "# Parameters\n",
    "threshold = 0.25\n",
    "z = 0.5\n",
    "\n",
    "for study_n in range(0, 8):\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_vector)):\n",
    "        print(f\"\\nüîÅ Study {study_n + 1} - Optimizing fold {fold + 1}/{N_SPLITS}...\")\n",
    "\n",
    "        # Prepare fold data\n",
    "        fold_train = [train_vector[i] for i in train_idx]\n",
    "        fold_val = train_vector[val_idx[0]]\n",
    "        train_ids = train_df[train_df['fold'].isin(fold_train)]\n",
    "        val_ids = train_df.loc[train_df['fold'] == fold_val]\n",
    "\n",
    "        X_train_fold = train_ids.drop(columns=['fold', 'mos'])\n",
    "        y_train_fold = train_ids['mos']\n",
    "        X_val_fold = val_ids.drop(columns=['fold', 'mos'])\n",
    "        y_val_fold = val_ids['mos']\n",
    "        best_score = float(\"inf\")\n",
    "\n",
    "        def fold_objective(trial):\n",
    "\n",
    "            global best_score\n",
    "            \n",
    "            lgb_model = LGBMRegressor(\n",
    "                random_state=SEED + study_n * 10 + fold,\n",
    "                verbosity=-1,\n",
    "                objective='regression',\n",
    "                boosting_type='gbdt',\n",
    "\n",
    "                # Narrowed search for faster convergence\n",
    "                n_estimators=trial.suggest_int('n_estimators', 100, 200),\n",
    "                learning_rate=trial.suggest_float('learning_rate', 0.01, 1, log=True),\n",
    "\n",
    "                # Focused regularization\n",
    "                reg_alpha=trial.suggest_float('reg_alpha', 1e-3, 0.5, log=True),\n",
    "                reg_lambda=trial.suggest_float('reg_lambda', 1e-3, 0.5, log=True),\n",
    "\n",
    "                # Tight complexity control\n",
    "                max_depth=trial.suggest_int('max_depth', 3, 7),\n",
    "                num_leaves=trial.suggest_int('num_leaves', 6, 24),\n",
    "\n",
    "                # Sampling (still some flexibility)\n",
    "                colsample_bytree=trial.suggest_float('colsample_bytree', 0.75, 1.0),\n",
    "                subsample=trial.suggest_float('subsample', 0.7, 0.9),\n",
    "                subsample_freq=1,\n",
    "\n",
    "                # Leaf-wise control (slightly narrower)\n",
    "                min_child_samples=trial.suggest_int('min_child_samples', 20, 50),\n",
    "\n",
    "                # Optional new tuning dimensions:\n",
    "                max_bin=trial.suggest_categorical('max_bin', [255, 512, 768]),\n",
    "                min_split_gain=trial.suggest_float('min_split_gain', 0.0, 0.1),\n",
    "            )            \n",
    "            \n",
    "            lgb_model.fit(\n",
    "                X_train_fold,\n",
    "                y_train_fold,\n",
    "                #sample_weight=smooth_weights(y_train_fold.values),\n",
    "                eval_set=[(X_val_fold, y_val_fold)],\n",
    "                eval_metric='rmse',\n",
    "                callbacks=[early_stopping(stopping_rounds=30,  verbose=False)])\n",
    "            preds = lgb_model.predict(X_val_fold)\n",
    "            score = mean_squared_error(y_val_fold, preds)\n",
    "\n",
    "            # Save best model\n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                joblib.dump(lgb_model, f\"{MODEL_DIR}/lgbm_model_fold_mse_{fold+1}_{study_n+1}_3.pkl\")\n",
    "                \n",
    "            return score\n",
    "\n",
    "        # Run optimization for the current fold\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(fold_objective, n_trials=1000, n_jobs=1)\n",
    "        best_params = study.best_trial.params\n",
    "        best_model = joblib.load(f\"{MODEL_DIR}/lgbm_model_fold_mse_{fold+1}_{study_n+1}_3.pkl\")  \n",
    "        preds_best = best_model.predict(X_val_fold)\n",
    "\n",
    "        print(f\"‚úÖ Best mse: {mean_squared_error(y_val_fold, preds_best):.4f}\")\n",
    "        print(f\"‚úÖ Fold {fold + 1} best mse: {study.best_value:.4f}\")\n",
    "        print(f\"üõ†Ô∏è  Best hyperparameters: {best_params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
