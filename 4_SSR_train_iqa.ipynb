{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the script, the visdom visualization tool must be initialized. Follow the following instructions:\n",
    "\n",
    "* Open a new PowerShell or Bash window\n",
    "* Go to the project repository\n",
    "* Activate the python environment: `.\\.venv\\Scripts\\activate`\n",
    "* Execute: `python -m visdom.server`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T11:29:49.062614Z",
     "start_time": "2025-03-27T11:29:45.145327Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install tifffile\n",
    "#!pip install einops\n",
    "#!pip install visdom\n",
    "#!pip install scikit-image\n",
    "#!pip install torchinfo\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import gc\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm import tqdm\n",
    "from models.iqa_module_baseline import mymodel\n",
    "from utils.dataset import MyDataset_xinbo\n",
    "import visdom\n",
    "from random import sample\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T11:29:50.866882Z",
     "start_time": "2025-03-27T11:29:50.851255Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def seed_torch(seed):\n",
    "    random.seed(seed) # python seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed) # 设置python哈希种子，for certain hash-based operations (e.g., the item order in a set or a dict）。seed为0的时候表示不用这个feature，也可以设置为整数。 有时候需要在终端执行，到脚本实行可能就迟了。\n",
    "    np.random.seed(seed) # If you or any of the libraries you are using rely on NumPy, 比如Sampling，或者一些augmentation。 哪些是例外可以看https://pytorch.org/docs/stable/notes/randomness.html\n",
    "    torch.manual_seed(seed) # 为当前CPU设置随机种子。 pytorch官网倒是说(both CPU and CUDA)\n",
    "    torch.cuda.manual_seed(seed) # 为当前GPU设置随机种子\n",
    "    # torch.cuda.manual_seed_all(seed) # 使用多块GPU时，均设置随机种子\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    # torch.backends.cudnn.benchmark = True # 设置为True时，cuDNN使用非确定性算法寻找最高效算法\n",
    "    # torch.backends.cudnn.enabled = True # pytorch使用CUDANN加速，即使用GPU加速\n",
    "seed_torch(seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mymodel()\n",
    "model = model.to(torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# Print summary\n",
    "summary(model,\n",
    "        input_size=(1,3,480, 270),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some constants\n",
    "NUM_WORKERS = 0\n",
    "AMOUNT_TO_GET = 1.0\n",
    "SEED = 42\n",
    "BATCH_SIZE = 2\n",
    "N_SPLITS = 8\n",
    "NUM_EPOCHS = 80\n",
    "ACCUM_STEPS = 2\n",
    "\n",
    "# Define target data directory\n",
    "BASELINE_NAME = f\"VCIP_IMQA/VCIP\"\n",
    "BASELINE = Path(BASELINE_NAME)\n",
    "TARGET_DIR = BASELINE / \"EQ420_image\"\n",
    "TARGET_LABEL = BASELINE / \"Labels\"\n",
    "TARGET_BASE = BASELINE / \"IMQA\"\n",
    "TARGET_DIR = r'VCIP_IMQA/VCIP/EQ420_image/'\n",
    "\n",
    "# Create target model directory\n",
    "MODEL_DIR = Path(\"outputs\")\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Read train csv\n",
    "idx_csv = pd.read_csv(TARGET_LABEL / 'mos_fold_train.csv').sample(frac=1)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Check out splitting\n",
    "fold_vector = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(fold_vector)):\n",
    "    print(train_idx, val_idx)\n",
    "\n",
    "#df = pd.DataFrame(0, index=[f\"folder {i}\" for i in fold_vector], columns=['plcc', 'srocc'])\n",
    "#df = pd.read_csv(f\"plcc_srocc_baseline_folds{N_SPLITS}.csv\")\n",
    "df = pd.DataFrame(0, index=[f\"folder {i}\" for i in fold_vector] + [\"average\"] + [\"global\"], columns=['plcc', 'srocc', 'rmse'])\n",
    "\n",
    "#display(df.head(11))\n",
    "\n",
    "for error in ['mse']: #huber\n",
    "    for opt in ['adam']: #, 'adamw']:\n",
    "        \n",
    "        # Initialize result dataframe\n",
    "        df = pd.DataFrame(0, index=[f\"folder {i}\" for i in fold_vector] + [\"average\"] + [\"global\"], columns=['plcc', 'srocc', 'rmse'])\n",
    "        csv_file_name = f\"plcc_srocc_baseline_folds{N_SPLITS}_{opt}_{error}.csv\"\n",
    "\n",
    "        # Execute K-folds\n",
    "        all_preds = []\n",
    "        all_gtmos = []\n",
    "        kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(fold_vector)):\n",
    "\n",
    "            #if (fold + 1) <= 3:\n",
    "            #    continue\n",
    "\n",
    "            # K-fold preparation\n",
    "            fold_train = [fold_vector[i] for i in train_idx]\n",
    "            fold_val = fold_vector[val_idx[0]]\n",
    "            train_ids = idx_csv[idx_csv['folds'].isin(fold_train)]\n",
    "            val_ids = idx_csv.loc[idx_csv['folds'] == fold_val]\n",
    "\n",
    "            \n",
    "            train_set = MyDataset_xinbo(ids=train_ids, ref_dir=TARGET_DIR)\n",
    "            val_set = MyDataset_xinbo(ids=val_ids, ref_dir=TARGET_DIR)\n",
    "            dataloaders = {'train':DataLoader(train_set, batch_size=BATCH_SIZE,shuffle=True, num_workers=4)\n",
    "                        ,'val':DataLoader(val_set, batch_size=BATCH_SIZE,shuffle=False, num_workers=4)}\n",
    "            \n",
    "            dataset_sizes = {'train':len(train_ids),'val':len(val_ids)}\n",
    "            \n",
    "            model = mymodel()\n",
    "            model = model.to(device)\n",
    "\n",
    "            port = 8097\n",
    "            viz = visdom.Visdom(port=port)\n",
    "            win = viz.scatter(X=np.asarray([[0,0]]))    \n",
    "            viz2 = visdom.Visdom(port=port)    \n",
    "            viz2.line(np.asarray([[0,0]]), np.asarray([[0,0]]), win='total_loss', opts=dict(title='total_loss', legend=['val', 'train']))\n",
    "            viz_line = visdom.Visdom(port=port)\n",
    "            viz_line.line([0.], [0.], win='train_loss', opts=dict(title='train loss'))\n",
    "            viz_line_v = visdom.Visdom(port=port)\n",
    "            viz_line_v.line([0.], [0.], win='val_loss', opts=dict(title='val loss'))\n",
    "\n",
    "            if opt == 'adam':\n",
    "                optimizer = optim.Adam(model.parameters(),lr=5e-6)\n",
    "            else:\n",
    "                optimizer = optim.AdamW(model.parameters(),lr=1e-3)\n",
    "            \n",
    "            scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "            # criterion1 = create_lossfunc()\n",
    "            if error == 'mse':\n",
    "                err = nn.MSELoss()\n",
    "            else:\n",
    "                err = nn.HuberLoss()\n",
    "            #bce = nn.BCELoss()\n",
    "            #HuberLoss = HuberLoss()\n",
    "\n",
    "            '''Training'''\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            best_loss = 1000000\n",
    "\n",
    "            #for k,v in model.named_parameters():\n",
    "            #    print('{}: {}'.format(k, v.requires_grad))\n",
    "\n",
    "            global_step_t = 0\n",
    "            global_step_v = 0\n",
    "\n",
    "            for epoch in range(NUM_EPOCHS):\n",
    "                print('Epoch {}/{}'.format(epoch + 1, NUM_EPOCHS))\n",
    "                print('-' * 10)\n",
    "\n",
    "                # Each epoch has a training and validation phase\n",
    "                for phase in ['train', 'val']:\n",
    "                    if phase == 'train':\n",
    "                        model.train()  # Set model to training mode\n",
    "                    else:\n",
    "                        model.eval()   # Set model to evaluate mode\n",
    "\n",
    "                    running_loss = 0.0\n",
    "\n",
    "                    # Iterate over data\n",
    "                    optimizer.zero_grad()\n",
    "                    for i_batch, sample_batched in tqdm(enumerate(dataloaders[phase])):\n",
    "\n",
    "                        ref, mos = sample_batched['ref'], sample_batched['mos']\n",
    "                        ref, mos = ref.type(torch.cuda.FloatTensor), mos.type(torch.cuda.FloatTensor)\n",
    "                        ref, mos = ref.to(device), mos.to(device)\n",
    "\n",
    "                        # zero the parameter gradients\n",
    "                        #optimizer.zero_grad()\n",
    "\n",
    "                        # forward\n",
    "                        with torch.set_grad_enabled(phase == 'train'):\n",
    "                            pred_score  = model(ref)\n",
    "                            if error == 'mse':\n",
    "                                loss = err(pred_score.squeeze(), mos.squeeze())\n",
    "                            else:\n",
    "                                loss = err(pred_score, mos)\n",
    "                            #loss = HuberLoss(pred_score, mos)\n",
    "\n",
    "                            loss = loss / ACCUM_STEPS\n",
    "\n",
    "                            # backward + optimize only if in training phase\n",
    "                            if phase == 'train':\n",
    "                                loss.backward()\n",
    "                                #optimizer.step()\n",
    "\n",
    "                        # Reset batch accumulation\n",
    "                        if (i_batch + 1) % ACCUM_STEPS == 0 or (i_batch + 1) == dataset_sizes[phase]:\n",
    "                            optimizer.step()\n",
    "                            optimizer.zero_grad()\n",
    "\n",
    "                        # statistics\n",
    "                        running_loss += loss.item() * ref.size(0) * ACCUM_STEPS\n",
    "                        if phase == 'train':  \n",
    "                            viz_line.line([loss.item()], [global_step_t], win='train_loss', update='append')             \n",
    "                            global_step_t += 1\n",
    "                        elif phase == 'val':\n",
    "                            viz_line_v.line([loss.item()], [global_step_v], win='val_loss', update='append')\n",
    "                            global_step_v += 1\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        scheduler.step()\n",
    "\n",
    "                    epoch_loss = running_loss / dataset_sizes[phase]\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        viz.scatter(X=np.array([[epoch,epoch_loss]]),\n",
    "                                    name=\"train\",\n",
    "                                    win=win,\n",
    "                                    update=\"append\")\n",
    "                        epoch_loss_v = epoch_loss\n",
    "\n",
    "                    elif phase == 'val':\n",
    "                        viz.scatter(X=np.array([[epoch,epoch_loss]]),\n",
    "                                    name=\"val\",\n",
    "                                    win=win,\n",
    "                                    update=\"append\")\n",
    "                        epoch_loss_t = epoch_loss\n",
    "\n",
    "                    if phase == 'val':\n",
    "                        viz2.line(np.asarray([[epoch_loss_t, epoch_loss_v]]), np.asarray([[epoch,epoch]]), win='total_loss', opts=dict(title='total_loss', legend=['val','train']), update='append')\n",
    "\n",
    "                    #print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
    "\n",
    "                    if phase == 'val' and epoch_loss < best_loss:\n",
    "                        best_loss = epoch_loss\n",
    "                        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                        counter = 0\n",
    "                    elif phase == 'val' and epoch_loss >= best_loss:\n",
    "                        counter += 1\n",
    "                        if counter ==2:\n",
    "                            print('early stopped!')\n",
    "                            break\n",
    "                else:\n",
    "                    continue\n",
    "                break\n",
    "                print()\n",
    "\n",
    "            print('Best val loss: {:4f}'.format(best_loss))\n",
    "            \n",
    "            val_dataloader = {'val':DataLoader(val_set, batch_size=1,shuffle=False, num_workers=4)}\n",
    "\n",
    "            model.eval()\n",
    "            res = []\n",
    "\n",
    "            for i_batch, sample_batched in tqdm(enumerate(val_dataloader[\"val\"])):\n",
    "                ref, mos = sample_batched['ref'], sample_batched['mos']\n",
    "                ref, mos = ref.type(torch.cuda.FloatTensor), mos.type(torch.cuda.FloatTensor)\n",
    "                ref, mos = ref.to(device), mos.to(device)\n",
    "                with torch.set_grad_enabled(False):\n",
    "                    pred_score  = model(ref)\n",
    "                    pred_score = pred_score.squeeze().cpu().item()\n",
    "                    res.append(pred_score)\n",
    "            \n",
    "            # Compute metrics  \n",
    "            gtmos = val_ids[\"mos\"].tolist()    \n",
    "            plcc, _ = pearsonr(res, gtmos)\n",
    "            srocc, _ = spearmanr(res, gtmos)\n",
    "            rmse = root_mean_squared_error(res, gtmos)\n",
    "\n",
    "            # Store metrics\n",
    "            df.iloc[fold] = [plcc, srocc, rmse] \n",
    "\n",
    "            # Accumulate all predictions and ground truths\n",
    "            all_preds.extend(res)\n",
    "            all_gtmos.extend(gtmos)\n",
    "\n",
    "            df.to_csv(csv_file_name,index=False)\n",
    "\n",
    "            model.load_state_dict(best_model_wts)\n",
    "            #savepath = './outputs/iqa_total_20250616_' + 'fold' +str(fold) + error + opt\n",
    "            savepath = MODEL_DIR / f\"iqa_total_20250616_fold{fold}{error}{opt}\"\n",
    "            torch.save(model.state_dict(),savepath)\n",
    "\n",
    "            del model, best_model_wts\n",
    "            del train_set, val_set, dataloaders, val_dataloader, err, scheduler, optimizer\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        # Compute average metrics across folds\n",
    "        mean_plcc = df['plcc'].iloc[:8].mean()\n",
    "        mean_srocc = df['srocc'].iloc[:8].mean()\n",
    "        mean_rmse = df['rmse'].iloc[:8].mean()\n",
    "\n",
    "        # Compute global metrics from all predictions\n",
    "        global_plcc, _ = pearsonr(all_preds, all_gtmos)\n",
    "        global_srocc, _ = spearmanr(all_preds, all_gtmos)\n",
    "        global_rmse = root_mean_squared_error(all_preds, all_gtmos)\n",
    "\n",
    "        # Add a final row to the DataFrame\n",
    "        df.loc['average'] = [mean_plcc, mean_srocc, mean_rmse]\n",
    "        df.loc['global'] = [global_plcc, global_srocc, global_rmse]\n",
    "        df.to_csv(csv_file_name, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
